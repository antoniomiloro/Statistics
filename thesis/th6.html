<!DOCTYPE html>
<html>
    <head>
        <title>Statistics</title>
        <meta http-equiv='cache-control' content='no-cache'> 
        <meta http-equiv='expires' content='0'> 
        <meta http-equiv='pragma' content='no-cache'>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
        <style>
            body{
                background-color: beige;
                font-family:Arial, Helvetica, sans-serif;
            }

            .formula-box {
                border: 1px solid #ccc;
                padding: 20px;
                border-radius: 8px;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
                max-width: 600px;
                width: 100%;
            }
        </style>
    </head>
    <body>
        <a href="../index.html" style="display: block; text-align: center; font-size: 20px; color: blue; text-decoration: none;">Home</a>
        <h1>Th6: Algorithms for random variates generations</h1>
        
            Generating random variates from different probability distributions is a common task in simulation, statistical modeling, and various other fields. Here are some commonly used algorithms for generating random variates from specific distributions:
                <ol>
                    <li> <strong>Uniform Distribution (0, 1):</strong> <br>
                        Most programming languages provide a built-in function to generate random numbers between 0 and 1.
                    </li>
                    <li> <strong>Normal Distribution:</strong> <br>
                        Box-Muller Transformation: Given two independent random variables \( U_1, U_2 \) uniformly distributed on (0, 1), this algorithm generates two independent standard normal variables \( Z_0, Z_1 \) using the formulas:
                        \[ Z_0 = \sqrt{-2 \ln U_1} \cos(2\pi U_2) \]
                        \[ Z_1 = \sqrt{-2 \ln U_1} \sin(2\pi U_2) \]
                    </li>
                    <li> <strong>Exponential Distribution:</strong> <br>
                        Inverse Transform Method: Given a uniform random variable \( U \) on (0, 1), the exponential variate \( X \) can be generated using \( X = -\frac{1}{\lambda} \ln U \), where \( \lambda \) is the rate parameter.
                    </li>
                    <li> <strong>Poisson Distribution:</strong> <br>
                        Poisson Process: If \( U \) is a uniform random variable on (0, 1) and \( \lambda \) is the rate parameter, then the Poisson variate \( X \) can be generated by counting the number of events in a Poisson process until the cumulative distribution exceeds \( U \).
                    </li>
                    <li> <strong>Binomial Distribution:</strong> <br>
                        Repeated Bernoulli Trials: Generate \( n \) independent Bernoulli random variables with parameter \( p \), where \( n \) is the number of trials and \( p \) is the success probability.
                    </li>
                    <li> <strong>Cauchy Distribution:</strong> <br>
                        Inverse Transform Method: If \( U \) is a uniform random variable on (0, 1), the Cauchy variate can be generated using \( X = \tan(\pi(U - 0.5)) \).
                    </li>
                </ol>
        <h3><strong><u>Inverse Transform Method</u>:</strong></h3>
                The Inverse Transform Method is a common technique for generating random variates from a given probability distribution. <br>
                The basic idea behind this method is to start with a random variable \( U \) that is uniformly distributed between 0 and 1 and then use the inverse of the cumulative distribution function (CDF) of the desired distribution to transform \( U \) into a random variable with the desired distribution. <br>
                The steps involved in the Inverse Transform Method are:
                <ol>
                    <li><strong>Understand the Cumulative Distribution Function (CDF):</strong> The CDF, denoted by \( F(x) \), of a random variable \( X \) is the probability that \( X \) takes a value less than or equal to \( x \). Mathematically, \( F(x) = P(X \leq x) \).</li>
                    <li><strong>Find the Inverse of the CDF:</strong> The inverse CDF is denoted by \( F^{-1}(u) \), where \( u \) is a uniform random variable between 0 and 1. In other words, \( F^{-1}(u) \) gives the value \( x \) for which \( F(x) = u \).</li>
                    <li><strong>Generate a Uniform Random Variable:</strong> Generate a random variable \( U \) that is uniformly distributed between 0 and 1.</li>
                    <li><strong>Apply the Inverse CDF:</strong> Use the inverse CDF to transform \( U \) into a random variable \( X \) with the desired distribution: \( X = F^{-1}(U) \).</li>
                </ol>
                This method works because, by construction, \( F(X) \) has a uniform distribution between 0 and 1. 
            
                
            
        <h3><strong><u>Convolution Method</u>:</strong></h3>
                The Convolution Method is another technique for generating random variates from a specific probability distribution. This method is particularly useful for deriving the distribution of a sum of independent random variables. <br>
                The basic idea is to convolve the probability density functions (PDFs) of the individual random variables to obtain the PDF of their sum. <br>
                The steps involved in the Convolution Method are:
                <ol>
                    <li><strong>Identify Independent Random Variables:</strong> If you want to generate a random variate from the sum of independent random variables, identify those variables. Let's denote them as \(X_1, X_2, \ldots, X_n\).</li>
                    <li><strong>Determine the PDFs:</strong> Obtain the probability density functions (PDFs) of the individual random variables, denoted as \(f_1(x), f_2(x), \ldots, f_n(x)\).</li>
                    <li><strong>Formulate the Convolution:</strong> Compute the convolution of the PDFs, denoted as \(f_{X_1 + X_2 + \ldots + X_n}(x)\), which represents the PDF of the sum of the random variables. The convolution is given by:
                        <div class="formula-box">
                            \[ f_{X_1 + X_2 + \ldots + X_n}(x) = \int f_1(x_1) \cdot f_2(x - x_1) \cdot \ldots \cdot f_n(x - x_{n-1}) \,dx_1 \,dx_2 \ldots dx_{n-1} \]
                        </div>
                    </li>
                    <li><strong>Generate Random Variates:</strong> Once you have the PDF of the sum, you can generate random variates from it. This might involve using the Inverse Transform Method or other techniques depending on the complexity of the resulting distribution.</li>
                </ol>
                   
            <hr>
            <strong>Some resources:</strong> <br>
            (1) PDF. https://www.cyut.edu.tw/~hchorng/downdata/1st/SS8_Random%20Variate.pdf. <br>
            (2) Bookdown. https://bookdown.org/manuele_leonelli/SimBook/random-variate-generation.html#the-inverse-transform-method. <br>
            (3) omscs-notes. https://www.omscs-notes.com/simulation/random-variate-generation/.
            
        
    </body>
</html>